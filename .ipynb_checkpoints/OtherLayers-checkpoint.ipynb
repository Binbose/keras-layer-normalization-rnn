{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPool2D, Flatten, Multiply, Permute, Embedding, LSTM, Bidirectional\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, alignment_vector_size, return_sequence=False, **kwargs):\n",
    "        self.alignment_vector_size = alignment_vector_size\n",
    "        self.return_sequence = return_sequence\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_shape_ = input_shape\n",
    "\n",
    "        self.alignment_vector = self.add_weight(name=\"alignment_vector\", shape=(self.alignment_vector_size,), initializer='uniform', trainable=True)\n",
    "        self.kernel = self.add_weight(name=\"kernel\", shape=(self.alignment_vector_size, input_shape[2]), initializer='uniform', trainable=True)\n",
    "        self.bias = self.add_weight(name=\"bias\", shape=(self.alignment_vector_size,), initializer='uniform', trainable=True)\n",
    "        \n",
    "        super(MultiHeadAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, hidden_state_sequence): \n",
    "        hidden_state_sequence.set_shape(self.input_shape_)\n",
    "        \n",
    "        u = K.tanh( K.squeeze(K.dot(hidden_state_sequence, K.expand_dims(self.kernel)), axis=-1) + self.bias )\n",
    "        \n",
    "        aligned_u = K.squeeze(K.dot(u, K.expand_dims(self.alignment_vector)), axis=-1)\n",
    "        attention_weights = K.softmax( aligned_u, axis=1 )\n",
    "        \n",
    "        if self.return_sequence:\n",
    "            context_vector = K.expand_dims(attention_weights,axis=2) * hidden_state_sequence\n",
    "        else:\n",
    "            context_vector = K.sum( K.expand_dims(attention_weights,axis=2) * hidden_state_sequence, axis=1 )\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_sequence:\n",
    "            return input_shape\n",
    "        else:\n",
    "            return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.get_session().close()\n",
    "cfg = K.tf.ConfigProto()\n",
    "cfg.gpu_options.allow_growth = True\n",
    "K.set_session(K.tf.Session(config=cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_71 (InputLayer)        (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "embedding_71 (Embedding)     (None, 80, 128)           2560000   \n",
      "_________________________________________________________________\n",
      "bidirectional_67 (Bidirectio (None, 80, 256)           263168    \n",
      "_________________________________________________________________\n",
      "multi_head_attention_66 (Mul (None, 80, 256)           2580      \n",
      "_________________________________________________________________\n",
      "lstm_74 (LSTM)               (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 3,022,997\n",
      "Trainable params: 3,022,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, 128)(inputs)\n",
    "\n",
    "x = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(x)\n",
    "x = MultiHeadAttention(10, return_sequence=True)(x)\n",
    "x = LSTM(128, dropout=0.2, recurrent_dropout=0.2)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 442s 18ms/step - loss: 0.5082 - acc: 0.7497 - val_loss: 0.3919 - val_acc: 0.8255\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 421s 17ms/step - loss: 0.3650 - acc: 0.8432 - val_loss: 0.3807 - val_acc: 0.8358\n",
      "Epoch 3/15\n",
      "24992/25000 [============================>.] - ETA: 0s - loss: 0.3107 - acc: 0.8752"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=15, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
